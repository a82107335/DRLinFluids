{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active flow control of a 2D circular cylinder\n",
    "This is a example notebook for DRL-based active flow control in a 2D cylinder.\n",
    "For more details, see the paper [DRLinFluids: An open-source Python platform of coupling deep reinforcement learning and OpenFOAM](https://aip.scitation.org/doi/10.1063/5.0103113)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import the necessary modules and define the OpenFOAM environment class from the `drlinfluids.environments` module.\n",
    "\n",
    "For the definition of member functions, please refer to [Tensorforce documentation](https://tensorforce.readthedocs.io/en/latest/environments/environment.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "from time import time\n",
    "\n",
    "from drlinfluids.environments.tensorforce import OpenFoam\n",
    "from drlinfluids import utils as utils\n",
    "import drlinfluids\n",
    "\n",
    "class FlowAroundCylinder2D(OpenFoam):\n",
    "    def states(self):\n",
    "        if self.state_params['type'] == 'pressure':\n",
    "            return dict(\n",
    "                type='float',\n",
    "                shape=(int(self.state_params['probe_info'].shape[0]), )\n",
    "            )\n",
    "\n",
    "        elif self.state_params['type'] == 'velocity':\n",
    "            if self.foam_params['num_dimension'] == 2:\n",
    "                return dict(\n",
    "                    type='float',\n",
    "                    shape=(int(2 * self.state_params['probe_info'].shape[0]), )\n",
    "                )\n",
    "            elif self.foam_params['num_dimension'] == 3:\n",
    "                return dict(\n",
    "                    type='float',\n",
    "                    shape=(int(3 * self.state_params['probe_info'].shape[0]), )\n",
    "                )\n",
    "            else:\n",
    "                assert 0, 'Simulation type error!'\n",
    "\n",
    "        else:\n",
    "            assert 0, 'No define state type error!'\n",
    "\n",
    "    def actions(self):\n",
    "        return dict(\n",
    "            type='float',\n",
    "            shape=(1, ),\n",
    "            min_value=self.agent_params['minmax_value'][0],\n",
    "            max_value=self.agent_params['minmax_value'][1]\n",
    "        )\n",
    "\n",
    "    def agent_actions_decorator(self, actions):\n",
    "        if self.num_trajectory < 1.5:\n",
    "            new_action = 0.4 * actions\n",
    "        else:\n",
    "            new_action = np.array(self.decorated_actions_sequence[self.num_trajectory - 2]) \\\n",
    "                         + 0.4 * (np.array(actions) - np.array(self.decorated_actions_sequence[self.num_trajectory - 2]))\n",
    "        return new_action\n",
    "\n",
    "    \n",
    "    def execute(self, actions=None):\n",
    "        self.trajectory_start_time = time()\n",
    "        self.num_trajectory += 1\n",
    "        if actions is None:\n",
    "            print(\"carefully, no action given!\")\n",
    "\n",
    "        self.decorated_actions = self.agent_actions_decorator(actions)\n",
    "\n",
    "        self.actions_sequence = np.append(self.actions_sequence, actions)\n",
    "\n",
    "        if self.num_trajectory < 1.5  :\n",
    "            self.start_actions=[self.actions_sequence[0]]\n",
    "            self.end_actions = [self.actions_sequence[0]]\n",
    "        else:\n",
    "            self.start_actions=[self.actions_sequence[-2]]\n",
    "            self.end_actions=[self.actions_sequence[-1]]\n",
    "\n",
    "        start_time_float = np.around(\n",
    "            float(self.cfd_init_time) + (self.num_trajectory - 1) * self.agent_params['interaction_period'],\n",
    "            decimals=self.decimal\n",
    "        )\n",
    "        end_time_float = np.around(start_time_float + self.agent_params['interaction_period'], decimals=self.decimal)\n",
    "\n",
    "        self.start_time_filename, self.start_time_path = utils.get_current_time_path(self.foam_root_path)\n",
    "\n",
    "        utils.dict2foam(\n",
    "            self.start_time_path,\n",
    "            utils.actions2dict(self.agent_params['entry_dict_q0'], self.agent_params['variables_q0'], self.start_actions)\n",
    "        )\n",
    "\n",
    "        utils.dict2foam(\n",
    "            self.start_time_path,\n",
    "            utils.actions2dict(self.agent_params['entry_dict_q1'], self.agent_params['variables_q1'], self.end_actions)\n",
    "        )\n",
    "\n",
    "        start_time=[start_time_float]\n",
    "        utils.dict2foam(\n",
    "            self.start_time_path,\n",
    "            utils.actions2dict(self.agent_params['entry_dict_t0'], self.agent_params['variables_t0'], start_time)\n",
    "        )\n",
    "\n",
    "        \n",
    "        simulation_start_time = time()\n",
    "        drlinfluids.runner.run(\n",
    "            self.foam_root_path,\n",
    "            self.foam_params, self.agent_params['interaction_period'], self.agent_params['purgeWrite_numbers'],self.agent_params['writeInterval'],\n",
    "            self.agent_params['deltaT'],\n",
    "            start_time_float, end_time_float\n",
    "             )\n",
    "        simulation_end_time = time()\n",
    "\n",
    "        self.probe_velocity = utils.read_foam_file(\n",
    "            self.foam_root_path + f'/postProcessing/probes/{self.start_time_filename}/U',\n",
    "            dimension=self.foam_params['num_dimension']\n",
    "        )\n",
    "\n",
    "        self.probe_pressure = utils.read_foam_file(\n",
    "            self.foam_root_path + f'/postProcessing/probes/{self.start_time_filename}/p',\n",
    "            dimension=self.foam_params['num_dimension']\n",
    "        )\n",
    "\n",
    "        self.force = utils.resultant_force(\n",
    "            utils.read_foam_file(\n",
    "            self.foam_root_path + f'/postProcessing/forcesIncompressible/{self.start_time_filename}/forces.dat'\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.force_Coeffs = utils.read_foam_file(\n",
    "            self.foam_root_path + f'/postProcessing/forceCoeffsIncompressible/{self.start_time_filename}/forceCoeffs.dat'\n",
    "        )\n",
    "\n",
    "        if self.num_trajectory < 1.5:\n",
    "            self.history_force = self.force\n",
    "            self.history_force_Coeffs = self.force_Coeffs\n",
    "        else:\n",
    "            self.history_force = pd.concat([self.history_force, self.force[1:]]).reset_index(drop=True)\n",
    "            self.history_force_Coeffs = pd.concat(\n",
    "                [self.history_force_Coeffs, self.force_Coeffs[1:]]\n",
    "            ).reset_index(drop=True)\n",
    "\n",
    "\n",
    "        if self.state_params['type'] == 'pressure':\n",
    "            next_state = self.probe_pressure.iloc[-1, 1:].to_numpy()\n",
    "        elif self.state_params['type'] == 'velocity':\n",
    "            next_state = self.probe_velocity.iloc[-1, 1:].to_numpy()\n",
    "        else:\n",
    "            next_state = False\n",
    "            assert next_state, 'No define state type'\n",
    "\n",
    "        self.state_data = np.append(self.state_data, next_state)\n",
    "\n",
    "        self.decorated_actions_sequence = np.append(self.decorated_actions_sequence,  actions)\n",
    "\n",
    "        reward = self.reward_function()\n",
    "        self.trajectory_reward = np.append(self.trajectory_reward, reward)\n",
    "        self.episode_reward += reward\n",
    "        print(self.num_trajectory,self.start_actions,self.end_actions,reward)\n",
    "\n",
    "        terminal = False\n",
    "\n",
    "        self.trajectory_end_time = time()\n",
    "\n",
    "        self.exec_info = {\n",
    "            'episode': self.num_episode,\n",
    "            'trajectory': self.num_trajectory,\n",
    "            'start_time_float': start_time_float,\n",
    "            'end_time_float': end_time_float,\n",
    "            'timestampStart': self.trajectory_start_time,\n",
    "            'timestampEnd': self.trajectory_end_time,\n",
    "            'current_trajectory_reward': reward,\n",
    "            'episode_reward': self.episode_reward,\n",
    "            'actions': actions,\n",
    "            'cfd_running_time': simulation_end_time - simulation_start_time,\n",
    "            'number_cfd_timestep': int(np.around((end_time_float - start_time_float) / self.foam_params['delta_t'])),\n",
    "            'envName': self.foam_root_path.split('/')[-1],  #str\n",
    "            'current_state': self.state_data[-2],\n",
    "            'next_state': next_state\n",
    "        }\n",
    "        self.info_list.append(self.exec_info)\n",
    "        \n",
    "        return next_state, terminal, reward\n",
    "\n",
    "    def reward_function(self):\n",
    "        vortex_shedding_period = 0.025\n",
    "        drug_coeffs_sliding_average = drlinfluids.utils.force_coeffs_sliding_average(self.history_force_Coeffs, vortex_shedding_period, self.foam_params['delta_t'])[0]\n",
    "        lift_coeffs_sliding_average = self.force_coeffs_sliding_average(self.history_force_Coeffs, vortex_shedding_period, self.foam_params['delta_t'])[1]\n",
    "        return 3.205 - drug_coeffs_sliding_average - 0.1 * np.abs(lift_coeffs_sliding_average)\n",
    "\n",
    "\n",
    "    def force_coeffs_sliding_average(self, sliding_time_interval):\n",
    "        sampling_num = int(sliding_time_interval / self.foam_params['delta_t'])\n",
    "        if self.history_force_Coeffs.shape[0] <= sampling_num:\n",
    "            sliding_average_cd = np.mean(signal.savgol_filter(self.history_force_Coeffs.iloc[:, 2],49,0))\n",
    "            sliding_average_cl = np.mean(signal.savgol_filter(self.history_force_Coeffs.iloc[:, 3],49,0))\n",
    "        else:\n",
    "            sliding_average_cd = np.mean(signal.savgol_filter(self.history_force_Coeffs.iloc[-sampling_num:, 2],49,0))\n",
    "            sliding_average_cl = np.mean(signal.savgol_filter(self.history_force_Coeffs.iloc[-sampling_num:, 3],49,0))\n",
    "        return sliding_average_cd, sliding_average_cl\n",
    "\n",
    "\n",
    "    def sliding_history_force_coeffs(self, sliding_time_interval):\n",
    "        sampling_num = int(sliding_time_interval / self.foam_params['delta_t'])\n",
    "        if self.history_force_Coeffs.shape[0] <= sampling_num:\n",
    "            sliding_history_cd = self.history_force_Coeffs.iloc[:, 2]\n",
    "            sliding_history_cl = self.history_force_Coeffs.iloc[:, 3]\n",
    "        else:\n",
    "            sliding_history_cd = self.history_force_Coeffs.iloc[-sampling_num:, 2]\n",
    "            sliding_history_cl = self.history_force_Coeffs.iloc[-sampling_num:, 3]\n",
    "        return sliding_history_cd.to_numpy(), sliding_history_cl.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define the parameters of the environment, DRL agent, and the training process.\n",
    "\n",
    "Noted: `foam_params` needs to be defined according to the actual situation, especially for the `of_env_init` which aims to set proper OpenFOAM environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import re\n",
    "from tensorforce import Runner, Agent, Environment\n",
    "import os\n",
    "\n",
    "# define parameters\n",
    "number_servers=6\n",
    "nstate=1\n",
    "naction=1\n",
    "foam_params = {\n",
    "    'delta_t': 0.0005,\n",
    "    'solver': 'pimpleFoam',\n",
    "    'num_processor': 5,\n",
    "    'of_env_init': 'source ~/OpenFOAM/OpenFOAM-8/etc/bashrc',\n",
    "    'cfd_init_time': 0.005,\n",
    "    'num_dimension': 2,\n",
    "    'verbose': False\n",
    "}\n",
    "\n",
    "entry_dict_q0 = {\n",
    "    'U': {\n",
    "        'JET1': {\n",
    "            'q0': '{x}',\n",
    "        },\n",
    "        'JET2': {\n",
    "            'q0': '{-x}',\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "entry_dict_q1 = {\n",
    "    'U': {\n",
    "        'JET1': {\n",
    "            'q1': '{y}',\n",
    "        },\n",
    "        'JET2': {\n",
    "            'q1': '{-y}',\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "entry_dict_t0 = {\n",
    "    'U': {\n",
    "        'JET1': {\n",
    "            't0': '{t}'\n",
    "        },\n",
    "        'JET2': {\n",
    "            't0': '{t}'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "agent_params = {\n",
    "    'entry_dict_q0': entry_dict_q0,\n",
    "    'entry_dict_q1': entry_dict_q1,\n",
    "    'entry_dict_t0': entry_dict_t0,\n",
    "    'deltaA': 0.05,\n",
    "    'minmax_value': (-1.5, 1.5),\n",
    "    'interaction_period': 0.025,\n",
    "    'purgeWrite_numbers': 0,\n",
    "    'writeInterval': 0.025,\n",
    "    'deltaT': 0.0005,\n",
    "    'variables_q0': ('x',),\n",
    "    'variables_q1': ('y',),\n",
    "    'variables_t0': ('t',),\n",
    "    'verbose': False,\n",
    "    \"zero_net_Qs\": True,\n",
    "}\n",
    "state_params = {\n",
    "    'type': 'velocity'\n",
    "}\n",
    "\n",
    "# Pre-defined or custom environment\n",
    "root_path = os.getcwd()\n",
    "env_name_list = sorted([envs for envs in os.listdir(root_path) if re.search(r'^env\\d+$', envs)])\n",
    "environments = []\n",
    "for env_name in env_name_list:\n",
    "    env = FlowAroundCylinder2D(\n",
    "        foam_root_path='/'.join([root_path, env_name]),\n",
    "        foam_params=foam_params,\n",
    "        agent_params=agent_params,\n",
    "        state_params=state_params,\n",
    "    )\n",
    "    environments.append(env)\n",
    "\n",
    "use_best_model = True\n",
    "if use_best_model:\n",
    "    evaluation_environment = environments.pop()\n",
    "else:\n",
    "    evaluation_environment = None\n",
    "\n",
    "network_spec = [\n",
    "    dict(type='dense', size=512,activation='tanh'),\n",
    "    dict(type='dense', size=512,activation='tanh')\n",
    "]\n",
    "baseline_spec = [   \n",
    "   dict(type='dense', size=512,activation='tanh'),\n",
    "    dict(type='dense', size=512,activation='tanh')\n",
    "]\n",
    "\n",
    "# Train for 1000 episodes, each episode contain 100 actions\n",
    "num_episodes = 1000\n",
    "max_episode_timesteps = 100\n",
    "\n",
    "# Instantiate a Tensorforce agent\n",
    "agent = Agent.create(\n",
    "    agent='ppo',\n",
    "    environment=env,max_episode_timesteps=max_episode_timesteps,\n",
    "    batch_size=20,\n",
    "     network=network_spec,\n",
    "    learning_rate=0.001,state_preprocessing=None,\n",
    "    entropy_regularization=0.01, likelihood_ratio_clipping=0.2, subsampling_fraction=0.2,\n",
    "    predict_terminal_values=True,\n",
    "    discount=0.97,\n",
    "    baseline=baseline_spec,\n",
    "    baseline_optimizer=dict(\n",
    "        type='multi_step',\n",
    "        optimizer=dict(\n",
    "            type='adam',\n",
    "            learning_rate=1e-3\n",
    "        ),\n",
    "        num_steps=5\n",
    "    ),\n",
    "    multi_step=25,\n",
    "    parallel_interactions=number_servers,\n",
    "    saver=dict(directory=os.path.join(os.getcwd(), 'saved_models/checkpoint'),frequency=1  \n",
    "    # save checkpoint every 100 updates\n",
    "    ),\n",
    "    summarizer=dict(\n",
    "        directory='summary',\n",
    "        # list of labels, or 'all'\n",
    "        labels=['entropy', 'kl-divergence', 'loss', 'reward', 'update-norm']\n",
    "    ),\n",
    ")\n",
    "# print(agent.get_architecture())\n",
    "# agent = Agent.load(directory='saved_models/checkpoint')\n",
    "\n",
    "print('Agent defined DONE!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Start trainning the DRL agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = Runner(\n",
    "    agent=agent,\n",
    "    environments=environments,\n",
    "    max_episode_timesteps=max_episode_timesteps,\n",
    "    evaluation=use_best_model,\n",
    "    remote='multiprocessing',\n",
    ")\n",
    "print('Runner defined DONE!')\n",
    "\n",
    "# runner.run(episodes=500, max_episode_timesteps=80)\n",
    "runner.run(\n",
    "    num_episodes=num_episodes,\n",
    "    save_best_agent ='best_model'\n",
    ")\n",
    "runner.close()\n",
    "\n",
    "for environment in environments:\n",
    "    environment.close()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('test_drlinfluids')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d0589e3d0d923e7f5f48337a6611f279a7754b283dd4b872e5cf3351618c62a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
